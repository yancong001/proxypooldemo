{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl_Runoob\n"
     ]
    }
   ],
   "source": [
    "d = {'crawl_Michael': 95, 'crawl_Bob': 75, 'crawl_Tracy': 85}\n",
    "d['__CrawlFunc__']= []\n",
    "abc=[('crawl_Google', 'www.google.com'), ('crawl_taobao', 'www.taobao.com'), ('crawl_Runoob', 'www.runoob.com')]\n",
    "# print(d.items())\n",
    "for k, v in abc:\n",
    "    if 'crawl_' in k:\n",
    "        d['__CrawlFunc__'].append(k)\n",
    "    # print(d)\n",
    "print(d['__CrawlFunc__'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('__module__', '__main__'), ('__qualname__', 'FreeProxyGetter'), ('get_raw_proxies', <function FreeProxyGetter.get_raw_proxies at 0x0000000004D19378>), ('crawl_ip181', <function FreeProxyGetter.crawl_ip181 at 0x0000000004D19400>), ('crawl_xicidaili', <function FreeProxyGetter.crawl_xicidaili at 0x0000000004D192F0>), ('crawl_ip3366', <function FreeProxyGetter.crawl_ip3366 at 0x0000000004D19D90>), ('crawl_daili66', <function FreeProxyGetter.crawl_daili66 at 0x0000000004D19488>), ('__CrawlFunc__', [])])\n['crawl_ip181', 'crawl_xicidaili', 'crawl_ip3366', 'crawl_daili66'] !!! __CrawlFunc__\n"
     ]
    }
   ],
   "source": [
    "class ProxyMetaclass(type):\n",
    "    \"\"\"\n",
    "        元类，在FreeProxyGetter类中加入\n",
    "        __CrawlFunc__和__CrawlFuncCount__\n",
    "        两个参数，分别表示爬虫函数，和爬虫函数的数量。\n",
    "    \"\"\"\n",
    "    def __new__(cls, name, bases, attrs):\n",
    "        count = 0\n",
    "        attrs['__CrawlFunc__'] = []\n",
    "        print(attrs.items())\n",
    "        for k, v in attrs.items():\n",
    "            if 'crawl_' in k:\n",
    "                attrs['__CrawlFunc__'].append(k)\n",
    "                count += 1\n",
    "        attrs['__CrawlFuncCount__'] = count\n",
    "        print(v,\"!!!\",k)\n",
    "        return type.__new__(cls, name, bases, attrs)\n",
    "\n",
    "\n",
    "class FreeProxyGetter(object, metaclass=ProxyMetaclass):\n",
    "    def get_raw_proxies(self, callback):\n",
    "        proxies = []\n",
    "        print('Callback', callback)\n",
    "        for proxy in eval(\"self.{}()\".format(callback)):\n",
    "            print('Getting', proxy, 'from', callback)\n",
    "            proxies.append(proxy)\n",
    "        return proxies\n",
    "\n",
    "    def crawl_ip181(self):\n",
    "        start_url = 'http://www.ip181.com/'\n",
    "        html = get_page(start_url)\n",
    "        ip_adress = re.compile('<tr.*?>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "        # \\s* 匹配空格，起到换行作用\n",
    "        re_ip_adress = ip_adress.findall(html)\n",
    "        for adress,port in re_ip_adress:\n",
    "            result = adress + ':' + port\n",
    "            yield result.replace(' ', '')\n",
    "\n",
    "\n",
    "    def crawl_xicidaili(self):\n",
    "        for page in range(1, 4):\n",
    "            start_url = 'http://www.xicidaili.com/wt/{}'.format(page)\n",
    "            html = get_page(start_url)\n",
    "            ip_adress = re.compile('<td class=\"country\"><img src=\"http://fs.xicidaili.com/images/flag/cn.png\" alt=\"Cn\" /></td>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "            # \\s* 匹配空格，起到换行作用\n",
    "            re_ip_adress = ip_adress.findall(html)\n",
    "            for adress, port in re_ip_adress:\n",
    "                result = adress+':'+ port\n",
    "                yield result.replace(' ', '')\n",
    "\n",
    "    def crawl_ip3366(self):\n",
    "        for page in range(1, 4):\n",
    "            start_url = 'http://www.ip3366.net/free/?stype=1&page={}'.format(page)\n",
    "            html = get_page(start_url)\n",
    "            ip_adress = re.compile('<tr>\\s*<td>(.*?)</td>\\s*<td>(.*?)</td>')\n",
    "            # \\s * 匹配空格，起到换行作用\n",
    "            re_ip_adress = ip_adress.findall(html)\n",
    "            for adress, port in re_ip_adress:\n",
    "                result = adress+':'+ port\n",
    "                yield result.replace(' ', '')\n",
    "\n",
    "\n",
    "    def crawl_daili66(self, page_count=4):\n",
    "        start_url = 'http://www.66ip.cn/{}.html'\n",
    "        urls = [start_url.format(page) for page in range(1, page_count + 1)]\n",
    "        for url in urls:\n",
    "            print('Crawling', url)\n",
    "            html = get_page(url)\n",
    "            if html:\n",
    "                doc = pq(html)\n",
    "                trs = doc('.containerbox table tr:gt(0)').items()\n",
    "                for tr in trs:\n",
    "                    ip = tr.find('td:nth-child(1)').text()\n",
    "                    port = tr.find('td:nth-child(2)').text()\n",
    "                    yield ':'.join([ip, port])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n1\n2\n3\n5\n8\nd\no\nn\ne\n"
     ]
    }
   ],
   "source": [
    "def fib(max):\n",
    "    n, a, b = 0, 0, 1\n",
    "    while n < max:\n",
    "        print (b)\n",
    "        a, b = b, a + b\n",
    "        n = n + 1b\n",
    "    return 'done'\n",
    "# print(fib(6))\n",
    "for proxy in eval(\"fib(6)\"):\n",
    "    print(proxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}